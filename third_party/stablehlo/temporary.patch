diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp
--- stablehlo/stablehlo/dialect/Base.cpp
+++ stablehlo/stablehlo/dialect/Base.cpp
@@ -680,6 +680,7 @@
           {{bf16, bf16, f32, 1}, KnownDotAlgorithm::BF16_BF16_F32},
           {{bf16, bf16, f32, 3}, KnownDotAlgorithm::BF16_BF16_F32_X3},
           {{bf16, bf16, f32, 6}, KnownDotAlgorithm::BF16_BF16_F32_X6},
+          {{bf16, bf16, f32, 9}, KnownDotAlgorithm::BF16_BF16_F32_X9},
           {{tf32, tf32, f32, 1}, KnownDotAlgorithm::TF32_TF32_F32},
           {{tf32, tf32, f32, 3}, KnownDotAlgorithm::TF32_TF32_F32_X3},
           {{f32, f32, f32, 1}, KnownDotAlgorithm::F32_F32_F32},
diff --ruN a/stablehlo/stablehlo/dialect/Base.h b/stablehlo/stablehlo/dialect/Base.h
--- stablehlo/stablehlo/dialect/Base.h
+++ stablehlo/stablehlo/dialect/Base.h
@@ -260,6 +260,7 @@
   TF32_TF32_F32_X3 = 10,
   F32_F32_F32 = 11,
   F64_F64_F64 = 12,
+  BF16_BF16_F32_X9 = 13,
 };
 
 FailureOr<KnownDotAlgorithm> getKnownDotAlgorithm(
diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.cpp b/stablehlo/stablehlo/dialect/ChloOps.cpp
--- stablehlo/stablehlo/dialect/ChloOps.cpp
+++ stablehlo/stablehlo/dialect/ChloOps.cpp
@@ -710,31 +710,6 @@
   return success();
 }
 
-LogicalResult RaggedDotOp::inferReturnTypes(
-    MLIRContext*, std::optional<Location>, ValueRange operands,
-    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,
-    SmallVectorImpl<Type>& inferredReturnTypes) {
-  RaggedDotOp::Adaptor op(operands, attributes, properties, regions);
-
-  auto rankedLhsType = cast<RankedTensorType>(op.getLhs().getType());
-  auto rankedRhsType = cast<RankedTensorType>(op.getRhs().getType());
-  auto rankedGroupSizesType =
-      cast<RankedTensorType>(op.getGroupSizes().getType());
-  auto raggedDotDimNums = op.getRaggedDotDimensionNumbers();
-
-  inferredReturnTypes.push_back(RankedTensorType::get(
-      inferRaggedDotOutputDimensions(
-          rankedLhsType, rankedRhsType, rankedGroupSizesType,
-          raggedDotDimNums.getLhsBatchingDimensions(),
-          raggedDotDimNums.getRhsBatchingDimensions(),
-          raggedDotDimNums.getLhsContractingDimensions(),
-          raggedDotDimNums.getRhsContractingDimensions(),
-          raggedDotDimNums.getLhsRaggedDimensions(),
-          raggedDotDimNums.getRhsGroupDimensions()),
-      rankedLhsType.getElementType()));
-  return success();
-}
-
 //===----------------------------------------------------------------------===//
 // TopKOp
 //===----------------------------------------------------------------------===//
diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.td b/stablehlo/stablehlo/dialect/ChloOps.td
--- stablehlo/stablehlo/dialect/ChloOps.td
+++ stablehlo/stablehlo/dialect/ChloOps.td
@@ -856,8 +856,7 @@
   let hasCustomAssemblyFormat = 1;
 }
 
-def CHLO_RaggedDotOp : CHLO_Op<"ragged_dot",
-    [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
+def CHLO_RaggedDotOp : CHLO_Op<"ragged_dot", [Pure]> {
   string summary = "Computes a matmul over a single ragged dimension";
 
   string description = [{
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
@@ -23,6 +23,7 @@
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/StringRef.h"
+#include "llvm/Support/Debug.h"
 #include "llvm/Support/ErrorHandling.h"
 #include "mlir/Dialect/CommonFolders.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
@@ -47,6 +48,8 @@
 #include "stablehlo/dialect/Base.h"
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/transforms/optimization/Passes.h"
+
+#define DEBUG_TYPE "stablehlo-optimization"
 
 namespace mlir {
 namespace stablehlo {
@@ -779,7 +782,12 @@
   using OpRewritePattern::OpRewritePattern;
   LogicalResult matchAndRewrite(IotaOp op,
                                 PatternRewriter& rewriter) const override {
+    LLVM_DEBUG(llvm::dbgs() << "EvalIotaOpPattern folding: " << op << '\n');
     auto resultType = cast<RankedTensorType>(op.getType());
+    size_t numElems = resultType.getNumElements();
+    if (numElems > kFoldOpEltLimit)
+      return rewriter.notifyMatchFailure(op, "too many elements to fold");
+
     auto elementType = resultType.getElementType();
 
     if (!elementType.isInteger())

